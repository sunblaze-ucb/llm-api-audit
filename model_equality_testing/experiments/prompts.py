"""
Library of functions to generate datasets of prompts.
The exposed functions are of the form get_{prompt}_prompts.
They return Datasets with these columns:
    
    - `plain` (str): the prompt without any model-specific modifications, and without the '"...' at the start of the model response.
        This is useful for APIs that apply chat templates automatically.
        Example:
        'Continue the sequence. Do not output anything else except the continuation of the sequence. Start the continuation immediately.\nSequence: 123'
    
    - `chat` (str): `plain` but formatted in the model's chat template, but WITHOUT the initial <s> token or the ending </s> token. 
        The goal is that later calls to the tokenizer with add_special_tokens=True will produce the same output as `chat_tokens`.
        Example for llama-2-7b-chat: 
            '[INST] Continue the sequence. Do not output anything else except the continuation of the sequence. Start the continuation immediately.\nSequence: 123 [/INST]'
            After tokenizer: '<s> [INST] Continue the sequence. Do not output anything else except the continuation of the sequence. Start the continuation immediately.\nSequence: 123 [/INST]'

    - `chat_with_special` (str): `chat` but keeping the special tokens
    
    - `chat_tokens`: the prompt with model-specific chat templates applied, including all <s> or </s> tokens, tokenized.
        This is for testing only, and should not be used as input to the model.
    
    - `chat_with_ellipses` (str): `chat` but with the ellipses at the end of the model response.
        The ellipses at the start of the model response, after the chat tempalte, helps chat models to start immediately instead of hedging.
        Whenever possible, we try to use this column instead of `chat`.
        This requires us to call the Completions endpoints for APIs. That's actually nice because it removes uncertainty about whether
        chat templates are being applied correctly by the API.
        Example for llama-2-7b-chat:
            '[INST] Continue the sequence. Do not output anything else except the continuation of the sequence. Start the continuation immediately.\nSequence: 123 [/INST] "...'

    - `chat_with_ellipses_special` (str): `chat_with_ellipses` but keeping the special tokens e.g. <s>
    
    - `chat_with_ellipses_tokens`: `chat_with_ellipses` tokenized.
        This is for testing only, and should not be used as input to the model.

    - `id`: a unique identifier for the prompt, generated by hashing the `plain` prompt.

    - `y`: for datasets where there is a gold response, this is the gold response.
"""

import os
import random
import datasets
from experiments.utils import seed_everything, StreamingDataset, hash_fn

# seed everything with seed 0 for consistent hashing
seed_everything(0)

#######################################

def _map_to_chat_format(ds, model, ellipses='"...'):
    return ds.map(
        lambda x: {
            "chat": model.format_as_chat(x["plain"], add_ellipses=False),
            "chat_with_special": model.format_as_chat(x["plain"], add_ellipses=False, remove_special=False),
            "chat_tokens": model.format_as_chat(x["plain"], tokenize=True, add_ellipses=False),
            "chat_with_ellipses": model.format_as_chat(x["plain"], add_ellipses=True, ellipses=ellipses),
            "chat_with_ellipses_special": model.format_as_chat(x["plain"], add_ellipses=True, remove_special=False, ellipses=ellipses),
            "chat_with_ellipses_tokens": model.format_as_chat(
                x["plain"], tokenize=True, add_ellipses=True, ellipses=ellipses
            ),
            "id": hash_fn(x["plain"]),
        }
    )

def _load_local_prompts(
    filename: str, num_prompts: int, num_digits: int, random_digit_fn: callable, model
):
    """
    Load prompts from a local file, or generate new ones if the file does not exist.
    """
    if os.path.exists(filename):
        with open(filename, "r") as f:
            prompts = f.readlines()
        prompts = [p.strip().replace("\\n", "\n") for p in prompts]
    else:
        prompts = []

    def random_x():
        number = str(random_digit_fn())
        for _ in range(num_digits - 1):
            number += str(random_digit_fn())
        return number

    if len(prompts) < num_prompts:
        prompts += [
            "Continue the sequence. Do not output anything else except the continuation of the sequence. Start the continuation immediately.\nSequence: "
            + random_x()
            for _ in range(num_prompts - len(prompts))
        ]

    with open(filename, "w") as f:
        for p in prompts:
            f.write(p.replace("\n", "\\n") + "\n")
    ds = datasets.Dataset.from_dict({"plain": prompts})
    ds = _map_to_chat_format(ds, model)
    print(f"{filename} prompts loaded: {ds['id']}")
    return ds

###########################

def get_dummy_prompts(model) -> datasets.Dataset:
    """Dummy prompts"""
    ds = datasets.Dataset.from_dict({"plain": ["hello", "world", "the sun", "the moon", "the stars"]})
    ds = _map_to_chat_format(ds, model)
    print(f"Dummy prompts loaded: {ds['id']}")
    return ds


def get_number_prompts(model):
    return _load_local_prompts(
        "number_prompts.txt", 100, 100, lambda: random.randint(0, 9), model
    )


def get_bit_prompts(model):
    return _load_local_prompts(
        "bit_prompts.txt", 100, 100, lambda: random.randint(0, 1), model
    )


def get_alphanumericpunct_prompts(model):
    ALPHANUMERIC_PUNCTUATION = (
        list(range(33, 48))  # !"#$%&'()*+,-./
        + list(range(58, 65))  # :;<=>?@
        + list(range(91, 97))  # [\]^_`
        + list(range(123, 127))  # {|}~
        + list(range(48, 58))  # 0123456789
        + list(range(65, 91))  # ABCDEFGHIJKLMNOPQRSTUVWXYZ
        + list(range(97, 123))  # abcdefghijklmnopqrstuvwxyz
    )
    ALPHANUMERIC_PUNCTUATION = [chr(i) for i in ALPHANUMERIC_PUNCTUATION]
    return _load_local_prompts(
        "alphanumericpunct_prompts.txt",
        100,
        100,
        lambda: random.choice(ALPHANUMERIC_PUNCTUATION),
        model,
    )

def _get_wikipedia_prompts(model, language: str) -> datasets.Dataset:
    """100-char Wikipedia snippets as prompts"""
    LANGUAGES = [
        "en", "de", "fr", "ru", "es", "it", "ceb", "uk", "ja", "nl"
    ]
    assert language in LANGUAGES, f"Language {language} not in {LANGUAGES}"

    ds = datasets.load_dataset("Cohere/wikipedia-2023-11-embed-multilingual-v3", language, split="train", streaming=True)

    def get_prompt(row):
        out = "Continue the paragraph. Do not output anything except the continuation to the paragraph. Start the continuation immediately.\n"
        out += '"' + row["text"][:100] + '..."'
        return out

    ds = ds.remove_columns(["url", "_id", "title","emb"]).map(
        lambda x: {
            "plain": get_prompt(x),
        }
    )
    ds = _map_to_chat_format(ds, model)
    print(f"Wikipedia prompts loaded in streaming mode")
    return StreamingDataset(ds, 100)

get_wikipedia_en_prompts = lambda model: _get_wikipedia_prompts(model, "en")
get_wikipedia_de_prompts = lambda model: _get_wikipedia_prompts(model, "de")
get_wikipedia_fr_prompts = lambda model: _get_wikipedia_prompts(model, "fr")
get_wikipedia_ru_prompts = lambda model: _get_wikipedia_prompts(model, "ru")
get_wikipedia_es_prompts = lambda model: _get_wikipedia_prompts(model, "es")
get_wikipedia_it_prompts = lambda model: _get_wikipedia_prompts(model, "it")
get_wikipedia_ceb_prompts = lambda model: _get_wikipedia_prompts(model, "ceb")
get_wikipedia_uk_prompts = lambda model: _get_wikipedia_prompts(model, "uk")
get_wikipedia_ja_prompts = lambda model: _get_wikipedia_prompts(model, "ja")
get_wikipedia_nl_prompts = lambda model: _get_wikipedia_prompts(model, "nl")

def get_humaneval_prompts(model) -> datasets.Dataset:
    """HumanEval prompts"""
    ds = datasets.load_dataset("openai/openai_humaneval", split="test", streaming=True)

    def get_prompt(row):
        out = "Complete the code. Do not output anything except the completion. Start the continuation immediately.\n"
        out += '```\n' + row["prompt"]
        return out

    ds = ds.map(
        lambda x: {
            "plain": get_prompt(x),
            "y": x["canonical_solution"],
        }
    )
    ds = _map_to_chat_format(ds, model, ellipses="```\n...\n")
    ds = ds.remove_columns(["entry_point", "test", "canonical_solution"])
    print(f"HumanEval prompts loaded in streaming mode")
    return StreamingDataset(ds, 100)

def get_ultrachat_prompts(model) -> datasets.Dataset:
    """Ultrachat_200K prompts"""
    ds = datasets.load_dataset("HuggingFaceH4/ultrachat_200k", split="test_gen", streaming=True)

    def get_prompt(row):
        out = row["prompt"]
        return out

    ds = ds.map(
        lambda x: {
            "plain": get_prompt(x),
        }
    )
    ds = _map_to_chat_format(ds, model, ellipses="Sure, here's what you asked for:")
    ds = ds.remove_columns(["prompt_id", "messages"])
    print(f"Ultrachat_200K prompts loaded in streaming mode")
    return StreamingDataset(ds, 100, char_limit=1000)

ANSWER_SEPARATOR = "|||"

def get_natural_questions_prompts(model) -> datasets.Dataset:
    """Natural Questions prompts"""
    ds = datasets.load_dataset("google-research-datasets/nq_open", split="validation", streaming=True)

    def get_prompt(row):
        out = "Answer the question. Do not output anything except the answer. Start the answer immediately.\n"
        out += row["question"]
        return out

    ds = ds.map(
        lambda x: {
            "plain": get_prompt(x),
            "y": ANSWER_SEPARATOR.join(x["answer"]),
        }
    )
    ds = ds.remove_columns(['answer'])
    ds = _map_to_chat_format(ds, model, ellipses="Answer:")
    print(f"Natural Questions prompts loaded in streaming mode")
    return StreamingDataset(ds, 100, char_limit=1000)

def get_cnn_prompts(model) -> datasets.Dataset:
    """CNN / DailyMail dataset"""
    ds = datasets.load_dataset("abisee/cnn_dailymail", "3.0.0", split="validation", streaming=True)

    def get_prompt(row):
        out = "Summarize the article in two sentences. Do not output anything except the summary. Start the summary immediately.\n\n"
        out += row["article"]
        return out

    ds = ds.map(
        lambda x: {
            "plain": get_prompt(x),
            "y": x["highlights"],
        }
    )
    ds = ds.remove_columns(["id"])
    ds = _map_to_chat_format(ds, model, ellipses="")
    print(f"CNN / DailyMail prompts loaded in streaming mode")
    return StreamingDataset(ds, 100, char_limit=10000)

def _get_mmlu_prompts(model, subset) -> datasets.Dataset:
    """MMLU prompts"""
    ds = datasets.load_dataset("cais/mmlu", subset, split="test", streaming=True)

    def get_prompt(row):
        out = "Answer the multiple choice question. Respond with 'A', 'B', 'C', or 'D' for your answer choice.\n\n"
        out += f"Question: {row['question']}\n"
        for choice, letter in zip(row["choices"], ["A", "B", "C", "D"]):
            out += f"{letter}. {choice}\n"
        out = out.strip()
        return out

    ds = ds.map(
        lambda x: {
            "plain": get_prompt(x),
        }
    )
    ds = _map_to_chat_format(ds, model, ellipses="Answer:")
    print(f"MMLU prompts loaded in streaming mode")
    return StreamingDataset(ds, 100, char_limit=1000)

get_mmlu_abstract_algebra_prompts = lambda model: _get_mmlu_prompts(model, "abstract_algebra")
get_mmlu_anatomy_prompts = lambda model: _get_mmlu_prompts(model, "anatomy")
get_mmlu_astronomy_prompts = lambda model: _get_mmlu_prompts(model, "astronomy")
get_mmlu_business_ethics_prompts = lambda model: _get_mmlu_prompts(model, "business_ethics")
get_mmlu_clinical_knowledge_prompts = lambda model: _get_mmlu_prompts(model, "clinical_knowledge")
get_mmlu_college_biology_prompts = lambda model: _get_mmlu_prompts(model, "college_biology")
get_mmlu_college_chemistry_prompts = lambda model: _get_mmlu_prompts(model, "college_chemistry")
get_mmlu_college_computer_science_prompts = lambda model: _get_mmlu_prompts(model, "college_computer_science")
get_mmlu_college_mathematics_prompts = lambda model: _get_mmlu_prompts(model, "college_mathematics")
get_mmlu_college_medicine_prompts = lambda model: _get_mmlu_prompts(model, "college_medicine")
get_mmlu_college_physics_prompts = lambda model: _get_mmlu_prompts(model, "college_physics")
get_mmlu_computer_security_prompts = lambda model: _get_mmlu_prompts(model, "computer_security")
get_mmlu_conceptual_physics_prompts = lambda model: _get_mmlu_prompts(model, "conceptual_physics")
get_mmlu_econometrics_prompts = lambda model: _get_mmlu_prompts(model, "econometrics")
get_mmlu_electrical_engineering_prompts = lambda model: _get_mmlu_prompts(model, "electrical_engineering")
get_mmlu_elementary_mathematics_prompts = lambda model: _get_mmlu_prompts(model, "elementary_mathematics")
get_mmlu_formal_logic_prompts = lambda model: _get_mmlu_prompts(model, "formal_logic")
get_mmlu_global_facts_prompts = lambda model: _get_mmlu_prompts(model, "global_facts")
get_mmlu_high_school_biology_prompts = lambda model: _get_mmlu_prompts(model, "high_school_biology")
get_mmlu_high_school_chemistry_prompts = lambda model: _get_mmlu_prompts(model, "high_school_chemistry")
get_mmlu_high_school_computer_science_prompts = lambda model: _get_mmlu_prompts(model, "high_school_computer_science")
get_mmlu_high_school_european_history_prompts = lambda model: _get_mmlu_prompts(model, "high_school_european_history")
get_mmlu_high_school_geography_prompts = lambda model: _get_mmlu_prompts(model, "high_school_geography")
get_mmlu_high_school_government_and_politics_prompts = lambda model: _get_mmlu_prompts(model, "high_school_government_and_politics")
get_mmlu_high_school_macroeconomics_prompts = lambda model: _get_mmlu_prompts(model, "high_school_macroeconomics")
get_mmlu_high_school_mathematics_prompts = lambda model: _get_mmlu_prompts(model, "high_school_mathematics")
get_mmlu_high_school_microeconomics_prompts = lambda model: _get_mmlu_prompts(model, "high_school_microeconomics")
get_mmlu_high_school_physics_prompts = lambda model: _get_mmlu_prompts(model, "high_school_physics")
get_mmlu_high_school_psychology_prompts = lambda model: _get_mmlu_prompts(model, "high_school_psychology")
get_mmlu_high_school_statistics_prompts = lambda model: _get_mmlu_prompts(model, "high_school_statistics")
get_mmlu_high_school_us_history_prompts = lambda model: _get_mmlu_prompts(model, "high_school_us_history")
get_mmlu_high_school_world_history_prompts = lambda model: _get_mmlu_prompts(model, "high_school_world_history")
get_mmlu_human_aging_prompts = lambda model: _get_mmlu_prompts(model, "human_aging")
get_mmlu_human_sexuality_prompts = lambda model: _get_mmlu_prompts(model, "human_sexuality")
get_mmlu_international_law_prompts = lambda model: _get_mmlu_prompts(model, "international_law")
get_mmlu_jurisprudence_prompts = lambda model: _get_mmlu_prompts(model, "jurisprudence")
get_mmlu_logical_fallacies_prompts = lambda model: _get_mmlu_prompts(model, "logical_fallacies")
get_mmlu_machine_learning_prompts = lambda model: _get_mmlu_prompts(model, "machine_learning")
get_mmlu_management_prompts = lambda model: _get_mmlu_prompts(model, "management")
get_mmlu_marketing_prompts = lambda model: _get_mmlu_prompts(model, "marketing")
get_mmlu_medical_genetics_prompts = lambda model: _get_mmlu_prompts(model, "medical_genetics")
get_mmlu_miscellaneous_prompts = lambda model: _get_mmlu_prompts(model, "miscellaneous")
get_mmlu_moral_disputes_prompts = lambda model: _get_mmlu_prompts(model, "moral_disputes")
get_mmlu_moral_scenarios_prompts = lambda model: _get_mmlu_prompts(model, "moral_scenarios")
get_mmlu_nutrition_prompts = lambda model: _get_mmlu_prompts(model, "nutrition")
get_mmlu_philosophy_prompts = lambda model: _get_mmlu_prompts(model, "philosophy")
get_mmlu_prehistory_prompts = lambda model: _get_mmlu_prompts(model, "prehistory")
get_mmlu_professional_accounting_prompts = lambda model: _get_mmlu_prompts(model, "professional_accounting")
get_mmlu_professional_law_prompts = lambda model: _get_mmlu_prompts(model, "professional_law")
get_mmlu_professional_medicine_prompts = lambda model: _get_mmlu_prompts(model, "professional_medicine")
get_mmlu_professional_psychology_prompts = lambda model: _get_mmlu_prompts(model, "professional_psychology")
get_mmlu_public_relations_prompts = lambda model: _get_mmlu_prompts(model, "public_relations")
get_mmlu_security_studies_prompts = lambda model: _get_mmlu_prompts(model, "security_studies")
get_mmlu_sociology_prompts = lambda model: _get_mmlu_prompts(model, "sociology")
get_mmlu_us_foreign_policy_prompts = lambda model: _get_mmlu_prompts(model, "us_foreign_policy")
get_mmlu_virology_prompts = lambda model: _get_mmlu_prompts(model, "virology")
get_mmlu_world_religions_prompts = lambda model: _get_mmlu_prompts(model, "world_religions")
